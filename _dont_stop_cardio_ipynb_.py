# -*- coding: utf-8 -*-
""""Dont_Stop_Cardio.ipynb"

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gNcPEPsl1Vx37byr6wv2oYe8JIjwtT03
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive/')
# %cd /content/drive/MyDrive/Cardio/

!pip install hrv-analysis

from hrvanalysis import remove_outliers, remove_ectopic_beats, interpolate_nan_values
from hrvanalysis import plot_poincare
from hrvanalysis import get_time_domain_features, plot_psd
from sklearn.preprocessing import MinMaxScaler

def get_inn(rr):
    scaler = MinMaxScaler()
    rr_intervals_without_outliers = remove_outliers(rr_intervals=rr,  
                                                low_rri=300, high_rri=2000)
    interpolated_rr_intervals = interpolate_nan_values(rr_intervals=rr_intervals_without_outliers,
                                                   interpolation_method="linear")
    nn_intervals_list = remove_ectopic_beats(rr_intervals=interpolated_rr_intervals, method="malik")
    interpolated_nn_intervals = interpolate_nan_values(rr_intervals=nn_intervals_list)
    normalized = scaler.fit(np.array(interpolated_nn_intervals).reshape(-1, 1)).transform(np.array(interpolated_nn_intervals).reshape(-1, 1))
    return normalized.tolist()

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import random

seed = 42
random.seed(seed)
np.random.seed(seed)

df = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')
sample_submission = pd.read_csv('sample_submission.csv')

from sklearn.model_selection import GroupKFold

group_kfold = GroupKFold(n_splits=5)

for train_index, test_index in group_kfold.split(df.x, df.y, df.id):
    #print("TRAIN:", train_index, "TEST:", test_index)
    X_train, X_test = df.iloc[train_index], df.iloc[test_index]
    y_train, y_test = df.y[train_index], df.y[test_index]

# X_train = df
# y_train = df['y']

sample_n = 13 # window length (odd)

"""# Подготовка данных"""

# перейти к нормированным интервалам
inns = []
for i in X_train.id.unique():
    inns = inns + get_inn(X_train[X_train['id'] == i].x)
X_train = X_train.reset_index(drop=True)
df_inn = X_train.join(pd.DataFrame(inns))
df_inn.columns = ['id', 'time', 'x', 'y', 'inn_x']

d = 0
x_vals = []
y_vals = []
current_id = 1
i = sample_n // 2
while i < len(df_inn) - sample_n // 2:
    if df_inn.id.iloc[i + sample_n // 2] != current_id:
        current_id = df_inn.id.iloc[i + sample_n // 2]
        #print(len(x_vals))
        i += sample_n - 1 
        continue
    x_vals.append(df_inn.inn_x.iloc[i - sample_n // 2: i+sample_n // 2 + 1].to_numpy())
    y_vals.append(df_inn.y.iloc[i])
    i += 1
x_vals = np.array(x_vals)

x_vals.shape

new_x_vals = x_vals.reshape((x_vals.shape[0], sample_n, 1))
y_vals = np.array(y_vals)

len(x_vals)

"""# Подготовка модели (простая сверточная сеть)"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dropout, MaxPooling1D

model = Sequential()
model.add(Conv1D(16, 3, input_shape=(sample_n, 1), activation='relu'))
model.add(Dropout(0.1))
model.add(Conv1D(32, 5, activation='relu'))
model.add(Dropout(0.1))
model.add(Conv1D(64, 5, activation='relu'))
model.add(GlobalMaxPooling1D())
model.add(Dense(256, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model.fit(new_x_vals, y_vals, epochs=100, validation_split=0.15)

"""# Подготовка тестовых данных"""

# перейти к нормированным nn-интервалам
inns = []
for i in X_test.id.unique():
    inns = inns + get_inn(X_test[X_test['id'] == i].x)
X_test = X_test.reset_index(drop=True)
df_inn_test = X_test.join(pd.DataFrame(inns))
df_inn_test.columns = ['id', 'time', 'x', 'y', 'inn_x']

general_pred = []
for j in df_inn_test.id.unique():
  y_pred = [0] * (sample_n // 2)
  subsample = df_inn_test[df_inn_test['id'] == j].inn_x
  i = sample_n // 2
  while i + sample_n // 2 < len(subsample):
    new_pred = model.predict(subsample[i - sample_n // 2: i + sample_n // 2 + 1].values.reshape(1, sample_n, 1))
    if new_pred > 0.5:
        y_pred.append(1)
    else:
        y_pred.append(0)
    i += 1
  y_pred.extend([y_pred[-1]]*(sample_n // 2))
  general_pred.extend(y_pred)

# Показатели на трэйне

from sklearn.metrics import f1_score
print("Common f-score = ", f1_score(X_test.y.values, general_pred))
print("Micro f-score = ", f1_score(X_test.y.values, general_pred, average='micro')) # it's a micro f1-score, not a common one!

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, plot_confusion_matrix

cm = confusion_matrix(X_test.y.values, general_pred)
disp = ConfusionMatrixDisplay (cm, ['No COVID', "COVID"])
disp.plot(values_format='d')

test['y'] = sample_submission['y']
inns = []
for i in test.id.unique():
    inns = inns + get_inn(test[test['id'] == i].x)
test = test.reset_index(drop=True)
df_inn_test = test.join(pd.DataFrame(inns))
df_inn_test.columns = ['id', 'time', 'x', 'y', 'inn_x']

general_pred = []
for j in df_inn_test.id.unique():
  y_pred = [0] * (sample_n // 2)
  subsample = df_inn_test[df_inn_test['id'] == j].inn_x
  i = sample_n // 2
  while i + sample_n // 2 < len(subsample):
    new_pred = model.predict(subsample[i - sample_n // 2: i + sample_n // 2 + 1].values.reshape(1, sample_n, 1))
    if new_pred > 0.5:
        y_pred.append(1)
    else:
        y_pred.append(0)
    i += 1
  y_pred.extend([y_pred[-1]]*(sample_n // 2))
  general_pred.extend(y_pred)

sample_submission['y'] = general_pred

sample_submission.y

sample_submission.to_csv('dont_stop_submission_obr_25.csv')